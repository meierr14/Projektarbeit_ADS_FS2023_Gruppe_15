{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-Zusammenfassung\n",
    "\n",
    "Der folgende Code führt verschiedene Operationen im Zusammenhang mit maschinellem Lernen und Datenanalysen durch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "Der Code importiert die folgenden Bibliotheken:\n",
    "\n",
    "- psycopg2: Eine Bibliothek zur Verbindung mit einer PostgreSQL-Datenbank.\n",
    "- pandas: Eine Bibliothek zur Datenmanipulation und -analyse.\n",
    "- matplotlib.pyplot: Eine Bibliothek zur Visualisierung von Daten mit Diagrammen.\n",
    "- sklearn.model_selection.train_test_split: Eine Funktion zum Aufteilen von Daten in Trainings- und Testsets.\n",
    "- sklearn.model_selection.GridSearchCV: Eine Funktion zur Durchführung einer Gitter-Suchlauf-Validierung für die Modellparameteroptimierung.\n",
    "- sklearn.ensemble.RandomForestClassifier: Ein Modellalgorithmus für Klassifikationen basierend auf Entscheidungsbäumen.\n",
    "- sklearn.metrics.accuracy_score: Eine Funktion zur Berechnung der Genauigkeit eines Klassifikationsmodells.\n",
    "- sklearn.metrics.classification_report: Eine Funktion zur Bereitstellung eines umfassenden Berichts über die Leistung eines Klassifikationsmodells.\n",
    "- sklearn.metrics.confusion_matrix: Eine Funktion zur Berechnung der Konfusionsmatrix eines Klassifikationsmodells.\n",
    "- tensorflow.keras.models.Sequential: Eine Klasse für den Aufbau von sequenziellen Modellen in Keras.\n",
    "- tensorflow.keras.layers.Dense: Eine Klasse für vollständig verbundene Schichten in einem Keras-Modell.\n",
    "\n",
    "#### Weitere Code-Operationen\n",
    "\n",
    "Der Code führt auch weitere Operationen durch, die nicht explizit importierte Bibliotheken erfordern. Diese Operationen können Daten laden, Modelltraining und -evaluation durchführen, Visualisierungen erstellen usw. Der genaue Inhalt dieser Operationen kann im Code selbst gefunden werden.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbindung zur Datenbank herstellen\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"PSQL_ADSFS2023Gruppe15\",\n",
    "    user=\"ADSFS2023Gruppe15\",\n",
    "    password=\"ADS_FS_2023_G15!?\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Queries für die DB Abfrage\n",
    "query_tabelle = \"SELECT team_id, mannschaft, punkte, tore FROM bundesliga_mannschaften\"\n",
    "query_resultate = \"SELECT matchday, id_teamh, id_teamg, tore_teamh, tore_teamg, winner_team_id FROM bundesliga_resultate\"\n",
    "query_shots_stats = \"SELECT matchday, team_id, shots_total FROM bundesliga_shots_stats\"\n",
    "query_duel_stats = \"SELECT matchday, team_id, duels_total, duels_won FROM bundesliga_duels_stats\"\n",
    "query_pass_stats = \"SELECT matchday, team_id, pass_complete, pass_failed, pass_total, pass_percentage FROM bundesliga_pass_stats\"\n",
    "query_corner_stats = \"SELECT matchday, team_id, corner_left, corner_right, corner_total FROM bundesliga_corners\"\n",
    "query_distance_stats = \"SELECT matchday, team_id, distance_total FROM bundesliga_distance_stats\"\n",
    "query_freekicks_stats = \"SELECT matchday, team_id, freekicks_total FROM bundesliga_freekicks\"\n",
    "query_touch_stats = \"SELECT matchday, team_id, touches_total FROM bundesliga_touch_stats\"\n",
    "\n",
    "# Daten aus DB Tabellen lesen und in einem Dataframe speichern\n",
    "df_tabelle = pd.read_sql(query_tabelle, conn)\n",
    "df_resultate = pd.read_sql(query_resultate, conn)\n",
    "df_shots = pd.read_sql(query_shots_stats, conn)\n",
    "df_duels = pd.read_sql(query_duel_stats, conn)\n",
    "df_pass = pd.read_sql(query_pass_stats, conn)\n",
    "df_corner = pd.read_sql(query_corner_stats, conn)\n",
    "df_distance = pd.read_sql(query_distance_stats, conn)\n",
    "df_freekicks = pd.read_sql(query_freekicks_stats, conn)\n",
    "df_touch = pd.read_sql(query_touch_stats, conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenfassen aller Dataframes in ein einziges Dataframe. Dies wird anhand der team_id gemacht, die in jeder Tabelle \n",
    "# vorhanden ist und als identifikator alles Teams gilt\n",
    "merged_df = pd.merge(df_resultate[['matchday', 'id_teamh', 'id_teamg', 'tore_teamh', 'tore_teamg', 'winner_team_id']],\n",
    "                     df_tabelle, left_on='id_teamh', right_on='team_id')\n",
    "merged_df = pd.merge(merged_df, df_tabelle, left_on='id_teamg', right_on='team_id', suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_shots, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_shots, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_duels, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_duels, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_pass, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_pass, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_corner, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_corner, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_distance, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_distance, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_freekicks, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_freekicks, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_touch, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_touch, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "\n",
    "#print(merged_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition von Merkmalen aus dem Datensetz die für das machinelle lernen benötigt werden\n",
    "# Die Merkmale dienen als Eingabevariable für das Lernmodell\n",
    "X = merged_df[['punkte_home', 'punkte_guest', 'tore_home', 'tore_guest',\n",
    "               'shots_total_home', 'shots_total_guest', 'duels_total_home', 'duels_total_guest',\n",
    "               'duels_won_home', 'duels_won_guest', 'pass_complete_home', 'pass_complete_guest',\n",
    "               'pass_failed_home', 'pass_failed_guest', 'pass_total_home', 'pass_total_guest',\n",
    "               'pass_percentage_home', 'pass_percentage_guest', 'corner_left_home','corner_left_guest',\n",
    "               'corner_right_home', 'corner_right_guest', 'corner_total_home', 'corner_total_guest',\n",
    "               'distance_total_home', 'distance_total_guest', 'freekicks_total_home', 'freekicks_total_guest',\n",
    "               'touches_total_home', 'touches_total_guest']]\n",
    "# Definition des Merkmals welches als Ausgabevariable\n",
    "y = merged_df['winner_team_id']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugen von neuen Spalten die das Lernmodell benötigt. Für jede neue Kategorie wird hier eine neue Spalte erstellt\n",
    "# In diesem Fall für jedes Team, welches in der Bundesliga mitmacht wird eine neue Spate erzeugt (1x als Heim und 1x als Gast Mannschaft)\n",
    "merged_df_dummies = pd.get_dummies(merged_df, columns=['mannschaft_home', 'mannschaft_guest'])\n",
    "\n",
    "# Durch das zusammenführen der Dataframes in einem vorgehenden Schritt werden einige Spalten doppelt aufgeführt. \n",
    "# Da diese unnötig sind werden sie hier entfernt\n",
    "merged_df_dummies.drop(['id_teamh', 'id_teamg', 'matchday', 'team_id_home', 'team_id_guest', 'duels_total_home', 'duels_total_guest'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = merged_df_dummies.drop(['winner_team_id'], axis=1)\n",
    "y = merged_df_dummies[['winner_team_id']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten werden in Trainings und Test Daten aufgeteilt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model wird trainiert\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Eine Vorhersage basierend auf den Testdaten machen\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>\n",
    "\n",
    "Model performance berechnen\n",
    "Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>\n",
    "\n",
    "Hyperparameter-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leistung des Modells durch Hyperparameter-Tuning verbessern. \n",
    "# Eine gängige Methode dafür ist GridSearchCV in Scikit-Learn, die wir hier verwenden:\n",
    "\n",
    "\n",
    "###### VERSION 1 ######\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "###### VERSION 2 ######\n",
    "# Das Modell, das wir optimieren wollen\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Die Hyperparameter, die wir ausprobieren möchten\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],   # Anzahl der Bäume\n",
    "    'max_depth': [None, 10, 20, 30],         # Maximale Tiefe der Bäume\n",
    "    'min_samples_split': [2, 5, 10],         # Minimale Anzahl von Samples, um einen internen Knoten zu teilen\n",
    "    'min_samples_leaf': [1, 2, 4]            # Minimale Anzahl von Samples, die an einem Blattknoten benötigt werden\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>\n",
    "Anwendung von GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>\n",
    "Ausgabe der besten Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters found: \", grid_search.best_params_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>\n",
    "Trainieren des Modells mit den besten Parametern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>\n",
    "Modell Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best = best_model.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f'Accuracy of best model: {accuracy_best:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>\n",
    "Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### VERSION 1 ######\n",
    "# Feature Importance analysieren, um zu verstehen, welche Merkmale am meisten zur Vorhersage beitragen:\n",
    "importance = model.feature_importances_\n",
    "for i, j in enumerate(importance):\n",
    "    print(X.columns[i], \"=\", j)\n",
    "\n",
    "\n",
    "###### VERSION 2 ######\n",
    "# Erhalte die Wichtigkeit der Merkmale\n",
    "importances = best_model.feature_importances_\n",
    "\n",
    "# Sortiere die Merkmale nach ihrer Wichtigkeit\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# Zeichne ein horizontales Balkendiagramm der Feature Importance\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>\n",
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "# Nachdem die Vorhersagen gemacht wurden:\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Verwenden der Funktion classification_report, um einen Textbericht über die wichtigsten Klassifikationsmetriken zu erstellen\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Verwenden der Funktion confusion_matrix, um eine Konfusionsmatrix zu erstellen\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Anzeigen der Konfusionsmatrix mit Hilfe von Matplotlib\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>\n",
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Ausführen einer 5-fache Kreuzvalidierung\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "# Durchschnittliche Genauigkeit über alle 5 Folds ausgeben\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titel\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "</span>\n",
    "neuronales Netzwerk mit TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuronales Netzwerk mit TensorFlow\n",
    "# Definieren des Modells\n",
    "model = Sequential()\n",
    "\n",
    "# Hinzufügen von Schichten\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid')) # Verwenden Sie 'softmax' für mehr als zwei Klassen\n",
    "\n",
    "# Kompilieren des Modells\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Verwenden Sie 'categorical_crossentropy' für mehr als zwei Klassen\n",
    "\n",
    "# Trainieren des Modells\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
