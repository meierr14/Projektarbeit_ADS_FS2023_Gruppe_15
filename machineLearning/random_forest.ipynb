{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-Zusammenfassung\n",
    "\n",
    "Der folgende Code führt verschiedene Operationen im Zusammenhang mit maschinellem Lernen und Datenanalysen durch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "Der Code importiert die folgenden Bibliotheken:\n",
    "\n",
    "- psycopg2: Eine Bibliothek zur Verbindung mit einer PostgreSQL-Datenbank.\n",
    "- pandas: Eine Bibliothek zur Datenmanipulation und -analyse.\n",
    "- matplotlib.pyplot: Eine Bibliothek zur Visualisierung von Daten mit Diagrammen.\n",
    "- sklearn.model_selection.train_test_split: Eine Funktion zum Aufteilen von Daten in Trainings- und Testsets.\n",
    "- sklearn.model_selection.GridSearchCV: Eine Funktion zur Durchführung einer Gitter-Suchlauf-Validierung für die Modellparameteroptimierung.\n",
    "- sklearn.ensemble.RandomForestClassifier: Ein Modellalgorithmus für Klassifikationen basierend auf Entscheidungsbäumen.\n",
    "- sklearn.metrics.accuracy_score: Eine Funktion zur Berechnung der Genauigkeit eines Klassifikationsmodells.\n",
    "- sklearn.metrics.classification_report: Eine Funktion zur Bereitstellung eines umfassenden Berichts über die Leistung eines Klassifikationsmodells.\n",
    "- sklearn.metrics.confusion_matrix: Eine Funktion zur Berechnung der Konfusionsmatrix eines Klassifikationsmodells.\n",
    "- tensorflow.keras.models.Sequential: Eine Klasse für den Aufbau von sequenziellen Modellen in Keras.\n",
    "- tensorflow.keras.layers.Dense: Eine Klasse für vollständig verbundene Schichten in einem Keras-Modell.\n",
    "\n",
    "#### Weitere Code-Operationen\n",
    "\n",
    "Der Code führt auch weitere Operationen durch, die nicht explizit importierte Bibliotheken erfordern. Diese Operationen können Daten laden, Modelltraining und -evaluation durchführen, Visualisierungen erstellen usw. Der genaue Inhalt dieser Operationen kann im Code selbst gefunden werden.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpsycopg2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split, GridSearchCV, cross_val_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verbindung zur Datenbank herstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"PSQL_ADSFS2023Gruppe15\",\n",
    "    user=\"ADSFS2023Gruppe15\",\n",
    "    password=\"ADS_FS_2023_G15!?\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Queries für die DB Abfrage\n",
    "#### Anschliessend die Daten aus den DB Tabellen lesen und in Dataframes speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tabelle = \"SELECT team_id, mannschaft, punkte, tore FROM bundesliga_mannschaften\"\n",
    "query_resultate = \"SELECT matchday, id_teamh, id_teamg, tore_teamh, tore_teamg, winner_team_id FROM bundesliga_resultate\"\n",
    "query_shots_stats = \"SELECT matchday, team_id, shots_total FROM bundesliga_shots_stats\"\n",
    "query_duel_stats = \"SELECT matchday, team_id, duels_total, duels_won FROM bundesliga_duels_stats\"\n",
    "query_pass_stats = \"SELECT matchday, team_id, pass_complete, pass_failed, pass_total, pass_percentage FROM bundesliga_pass_stats\"\n",
    "query_corner_stats = \"SELECT matchday, team_id, corner_left, corner_right, corner_total FROM bundesliga_corners\"\n",
    "query_distance_stats = \"SELECT matchday, team_id, distance_total FROM bundesliga_distance_stats\"\n",
    "query_freekicks_stats = \"SELECT matchday, team_id, freekicks_total FROM bundesliga_freekicks\"\n",
    "query_touch_stats = \"SELECT matchday, team_id, touches_total FROM bundesliga_touch_stats\"\n",
    "\n",
    "df_tabelle = pd.read_sql(query_tabelle, conn)\n",
    "df_resultate = pd.read_sql(query_resultate, conn)\n",
    "df_shots = pd.read_sql(query_shots_stats, conn)\n",
    "df_duels = pd.read_sql(query_duel_stats, conn)\n",
    "df_pass = pd.read_sql(query_pass_stats, conn)\n",
    "df_corner = pd.read_sql(query_corner_stats, conn)\n",
    "df_distance = pd.read_sql(query_distance_stats, conn)\n",
    "df_freekicks = pd.read_sql(query_freekicks_stats, conn)\n",
    "df_touch = pd.read_sql(query_touch_stats, conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zusammenfassen aller Dataframes in ein einziges Dataframe\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "Dies wird anhand der team_id gemacht, die in jeder Tabelle vorhanden ist und als identifikator alles Teams gilt\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_resultate[['matchday', 'id_teamh', 'id_teamg', 'tore_teamh', 'tore_teamg', 'winner_team_id']],\n",
    "                     df_tabelle, left_on='id_teamh', right_on='team_id')\n",
    "merged_df = pd.merge(merged_df, df_tabelle, left_on='id_teamg', right_on='team_id', suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_shots, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_shots, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_duels, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_duels, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_pass, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_pass, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_corner, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_corner, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_distance, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_distance, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_freekicks, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_freekicks, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "merged_df = pd.merge(merged_df, df_touch, left_on=['id_teamh', 'matchday'], right_on=['team_id', 'matchday'])\n",
    "merged_df = pd.merge(merged_df, df_touch, left_on=['id_teamg', 'matchday'], right_on=['team_id', 'matchday'], suffixes=('_home', '_guest'))\n",
    "\n",
    "#print(merged_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition der Merkmale\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "Definition von Merkmalen aus dem Datensetz die für das machinelle lernen benötigt werden. Die Merkmale dienen als Eingabevariable für das Lernmodell.\n",
    "Am Schluss wird das Merkmal definiert, das als Ausgabevariable dient.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df[['punkte_home', 'punkte_guest', 'tore_home', 'tore_guest',\n",
    "               'shots_total_home', 'shots_total_guest', 'duels_total_home', 'duels_total_guest',\n",
    "               'duels_won_home', 'duels_won_guest', 'pass_complete_home', 'pass_complete_guest',\n",
    "               'pass_failed_home', 'pass_failed_guest', 'pass_total_home', 'pass_total_guest',\n",
    "               'pass_percentage_home', 'pass_percentage_guest', 'corner_left_home','corner_left_guest',\n",
    "               'corner_right_home', 'corner_right_guest', 'corner_total_home', 'corner_total_guest',\n",
    "               'distance_total_home', 'distance_total_guest', 'freekicks_total_home', 'freekicks_total_guest',\n",
    "               'touches_total_home', 'touches_total_guest']]\n",
    "y = merged_df['winner_team_id']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorverarbeitung von Daten und Erstellen von Feature-Matrix und Zielvektor\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "Dieser Codeabschnitt ist mit der Vorverarbeitung der Daten und der Vorbereitung von Eingabe- und Ausgabedaten für ein maschinelles Lernmodell beschäftigt.\n",
    "\n",
    "1. Zuerst werden mit der Methode `get_dummies` neue Spalten für jede Kategorie in den Spalten 'mannschaft_home' und 'mannschaft_guest' erzeugt. Dies transformiert kategoriale Variablen in ein Format, das für maschinelles Lernen geeignet ist, indem es sogenannte Dummy-Variablen erstellt. Jedes Team der Bundesliga wird hierbei als Heim- und Gastmannschaft berücksichtigt.\n",
    "2. Anschliessend werden doppelte oder unnötige Spalten aus dem DataFrame entfernt, die durch vorheriges Zusammenführen von DataFrames entstanden sind.\n",
    "3. Schliesslich wird die Feature-Matrix `X` erstellt, indem die Spalte 'winner_team_id' aus dem DataFrame entfernt wird, und der Zielvektor `y` wird erstellt, indem nur die Spalte 'winner_team_id' ausgewählt wird.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugen von neuen Spalten die das Lernmodell benötigt. Für jede neue Kategorie wird hier eine neue Spalte erstellt\n",
    "# In diesem Fall für jedes Team, welches in der Bundesliga mitmacht wird eine neue Spate erzeugt (1x als Heim und 1x als Gast Mannschaft)\n",
    "merged_df_dummies = pd.get_dummies(merged_df, columns=['mannschaft_home', 'mannschaft_guest'])\n",
    "\n",
    "# Durch das zusammenführen der Dataframes in einem vorgehenden Schritt werden einige Spalten doppelt aufgeführt. \n",
    "# Da diese unnötig sind werden sie hier entfernt\n",
    "merged_df_dummies.drop(['id_teamh', 'id_teamg', 'matchday', 'team_id_home', 'team_id_guest', 'duels_total_home', 'duels_total_guest'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = merged_df_dummies.drop(['winner_team_id'], axis=1)\n",
    "y = merged_df_dummies[['winner_team_id']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufteilung der Daten, Modelltraining und Vorhersage\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "In diesem Codeabschnitt wird ein Klassifikationsmodell erstellt, trainiert und getestet.\n",
    "\n",
    "1. Zuerst werden die Daten in Trainings- und Testdatensätze aufgeteilt, wobei 70% der Daten zum Training und 30% zum Testen verwendet werden.\n",
    "2. Dann wird ein `RandomForestClassifier`-Modell erstellt und mit den Trainingsdaten trainiert.\n",
    "3. Schliesslich wird das trainierte Modell verwendet, um Vorhersagen auf den Testdatensatz zu machen.\n",
    "\n",
    "Dieser Codeabschnitt demonstriert die typischen Schritte beim Erstellen und Anwenden eines maschinellen Lernmodells, einschliesslich Datenaufteilung, Modelltraining und Vorhersage.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten werden in Trainings und Test Daten aufgeteilt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model wird trainiert\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Eine Vorhersage basierend auf den Testdaten machen\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter-Tuning / Berechnung und Ausgabe der Modellgenauigkeit\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "Dieser Codeabschnitt ist dafür verantwortlich, die Genauigkeit des trainierten Modells zu berechnen und auszugeben.\n",
    "\n",
    "1. Die Funktion `accuracy_score` aus der Bibliothek `sklearn.metrics` wird verwendet, um die Genauigkeit des Modells zu berechnen. Dies geschieht durch Vergleich der tatsächlichen Zielwerte im Testdatensatz (`y_test`) mit den vom Modell vorhergesagten Werten (`y_pred`).\n",
    "2. Die berechnete Genauigkeit wird dann als Fliesskommazahl mit zwei Nachkommastellen ausgegeben.\n",
    "\n",
    "Dieser Codeabschnitt stellt eine typische Methode dar, um die Leistung eines Klassifikationsmodells zu bewerten.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter-Tuning des Modells mit GridSearchCV\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "Dieser Codeabschnitt beschäftigt sich mit der Verbesserung der Leistung des Modells durch Hyperparameter-Tuning. Dafür wird `GridSearchCV` aus Scikit-Learn verwendet, eine Methode, die eine systematische Exploration einer Vielzahl möglicher Hyperparameterwerte ermöglicht.\n",
    "\n",
    "In beiden Versionen wird folgendermassen vorgegangen:\n",
    "\n",
    "1. Zuerst wird ein Raster (Grid) von Hyperparametern erstellt, das wir ausprobieren möchten. Dazu gehören die Anzahl der Schätzer (`n_estimators`), die maximale Tiefe (`max_depth`), die minimale Anzahl von Stichproben, um einen internen Knoten zu teilen (`min_samples_split`), und die minimale Anzahl von Stichproben, die an einem Blattknoten benötigt werden (`min_samples_leaf`).\n",
    "2. Dann wird `GridSearchCV` initialisiert mit dem Modell als Schätzer und dem Hyperparameter-Raster. `cv=3` legt fest, dass eine 3-fache Kreuzvalidierung durchgeführt wird, und `n_jobs=-1` ermöglicht es, alle Prozessoren zu verwenden.\n",
    "3. `GridSearchCV` wird dann auf den Trainingsdaten ausgeführt, um die beste Kombination von Hyperparametern zu finden.\n",
    "4. Schliesslich werden die besten gefundenen Parameter ausgegeben.\n",
    "\n",
    "In der Version 2 wird das `RandomForestClassifier` Modell erneut initialisiert, was den Code klarer macht, aber nicht notwendig ist, da das Modell bereits oben definiert wurde.\n",
    "\n",
    "Diese Codeblöcke zeigen den Prozess des Hyperparameter-Tunings, um die Leistung eines RandomForestClassifier-Modells zu optimieren.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### VERSION 1 ######\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "###### VERSION 2 ######\n",
    "# Das Modell, das wir optimieren wollen\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Die Hyperparameter, die wir ausprobieren möchten\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],   # Anzahl der Bäume\n",
    "    'max_depth': [None, 10, 20, 30],         # Maximale Tiefe der Bäume\n",
    "    'min_samples_split': [2, 5, 10],         # Minimale Anzahl von Samples, um einen internen Knoten zu teilen\n",
    "    'min_samples_leaf': [1, 2, 4]            # Minimale Anzahl von Samples, die an einem Blattknoten benötigt werden\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Durchführung von Hyperparameter-Tuning mittels GridSearchCV\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "Dieser Codeabschnitt führt das Hyperparameter-Tuning durch.\n",
    "\n",
    "1. Zuerst wird eine Instanz von `GridSearchCV` initialisiert. Hierbei wird das `RandomForestClassifier` Modell als Schätzer verwendet, und das zuvor definierte Hyperparameter-Raster wird übergeben. `cv=3` legt fest, dass eine 3-fache Kreuzvalidierung durchgeführt wird, und `n_jobs=-1` erlaubt die Verwendung aller verfügbaren Prozessoren.\n",
    "2. Dann wird `GridSearchCV` mit den Trainingsdaten gefüttert. Dabei wird `.values.ravel()` auf `y_train` angewendet, um eine eindimensionale Darstellung des Zielvektors zu gewährleisten, was häufig für Scikit-Learn Modelle erforderlich ist.\n",
    "\n",
    "Dieser Codeabschnitt stellt die Durchführung des Hyperparameter-Tunings mittels `GridSearchCV` dar, ein zentraler Schritt zur Optimierung der Leistung von maschinellen Lernmodellen.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ausgabe der besten Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters found: \", grid_search.best_params_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auswahl des besten Modells aus dem Hyperparameter-Tuning\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "Dieser Codeabschnitt dient dazu, das Modell mit der besten Leistung aus dem durchgeführten Hyperparameter-Tuning zu ermitteln.\n",
    "\n",
    "1. Die Methode `best_estimator_` von `GridSearchCV` wird verwendet, um das Modell mit den besten Hyperparametern zu extrahieren. Dieses Modell wird dann in der Variable `best_model` gespeichert.\n",
    "\n",
    "Dieser Codeabschnitt zeigt, wie man das beste Modell aus einem Hyperparameter-Tuning-Prozess mittels `GridSearchCV` ermittelt.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorhersage und Leistungsbewertung des besten Modells\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "Dieser Codeabschnitt macht Vorhersagen mit dem besten Modell aus dem Hyperparameter-Tuning und bewertet dessen Leistung.\n",
    "\n",
    "1. Zuerst verwendet das beste Modell, das von `GridSearchCV` gefunden wurde, die `predict` Methode, um Vorhersagen auf dem Testdatensatz zu machen. Diese Vorhersagen werden in `y_pred_best` gespeichert.\n",
    "2. Dann wird die Genauigkeit dieses Modells berechnet, indem `accuracy_score` verwendet wird, um die vorhergesagten Werte mit den tatsächlichen Werten zu vergleichen.\n",
    "3. Schliesslich wird die berechnete Genauigkeit als Fließkommazahl mit zwei Nachkommastellen ausgegeben.\n",
    "\n",
    "Dieser Codeabschnitt demonstriert die Anwendung und Leistungsbewertung des optimierten Modells.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best = best_model.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f'Accuracy of best model: {accuracy_best:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse der Feature Importance\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "In diesen Codeabschnitten wird die Feature Importance analysiert, um zu verstehen, welche Merkmale am meisten zur Vorhersage beitragen.\n",
    "\n",
    "#### VERSION 1 ####\n",
    "1. Zuerst werden die Wichtigkeiten der Merkmale des Modells extrahiert, indem `feature_importances_` verwendet wird.\n",
    "2. Dann wird für jedes Merkmal in der Sammlung der Merkmale der Wert der Feature Importance ausgegeben.\n",
    "\n",
    "#### VERSION 2 ####\n",
    "1. Ähnlich wie in Version 1, werden die Wichtigkeiten der Merkmale des besten Modells extrahiert.\n",
    "2. Dann wird eine Sortierung der Merkmale nach ihrer Wichtigkeit vorgenommen.\n",
    "3. Schliesslich wird ein horizontales Balkendiagramm erstellt, das die relative Wichtigkeit jedes Merkmals darstellt. Dies gibt eine visuelle Darstellung davon, welche Merkmale die grösste Rolle bei den Vorhersagen des Modells spielen.\n",
    "\n",
    "Diese Codeabschnitte illustrieren verschiedene Ansätze, um die Feature Importance in einem maschinellen Lernmodell zu analysieren.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### VERSION 1 ######\n",
    "# Feature Importance analysieren, um zu verstehen, welche Merkmale am meisten zur Vorhersage beitragen:\n",
    "importance = model.feature_importances_\n",
    "for i, j in enumerate(importance):\n",
    "    print(X.columns[i], \"=\", j)\n",
    "\n",
    "\n",
    "###### VERSION 2 ######\n",
    "# Erhalte die Wichtigkeit der Merkmale\n",
    "importances = best_model.feature_importances_\n",
    "\n",
    "# Sortiere die Merkmale nach ihrer Wichtigkeit\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# Zeichne ein horizontales Balkendiagramm der Feature Importance\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [X.columns[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modellbewertung\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "Dieser Codeabschnitt behandelt die Bewertung des Modells nachdem die Vorhersagen gemacht wurden.\n",
    "\n",
    "1. Zuerst werden Vorhersagen auf dem Testdatensatz mit dem Modell gemacht und in `y_pred` gespeichert.\n",
    "2. Danach wird die Funktion `classification_report` verwendet, um einen Textbericht zu erstellen, der die wichtigsten Klassifikationsmetriken enthält. Dieser Bericht wird ausgegeben und liefert eine Zusammenfassung der Genauigkeit, der Precision, des Recall und der F1-Score für jede Klasse.\n",
    "3. Anschliessend wird die Funktion `confusion_matrix` verwendet, um eine Konfusionsmatrix zu erstellen. Die Konfusionsmatrix (`cm`) gibt einen Überblick über die Anzahl der richtig und falsch klassifizierten Beobachtungen.\n",
    "4. Schliesslich wird die Konfusionsmatrix mit Hilfe von Matplotlib und Seaborn als Heatmap visualisiert. Auf der X-Achse sind die vorhergesagten Klassen und auf der Y-Achse die tatsächlichen Klassen dargestellt.\n",
    "\n",
    "Dieser Codeabschnitt zeigt, wie die Leistung eines Klassifikationsmodells bewertet und visualisiert werden kann.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nachdem die Vorhersagen gemacht wurden:\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Verwenden der Funktion classification_report, um einen Textbericht über die wichtigsten Klassifikationsmetriken zu erstellen\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Verwenden der Funktion confusion_matrix, um eine Konfusionsmatrix zu erstellen\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Anzeigen der Konfusionsmatrix mit Hilfe von Matplotlib\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Durchführung einer Kreuzvalidierung\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "Dieser Codeabschnitt führt eine Kreuzvalidierung durch, um die Robustheit des Modells zu bewerten.\n",
    "\n",
    "1. Ein `RandomForestClassifier` Modell wird initialisiert, wobei `n_estimators` auf 100 und `random_state` auf 42 gesetzt wird.\n",
    "2. Dann wird eine 5-fache Kreuzvalidierung auf den gesamten Datensatz ausgeführt, indem die `cross_val_score` Funktion verwendet wird. Die daraus resultierenden Genauigkeitswerte für die 5 Folds werden in `scores` gespeichert.\n",
    "3. Schliesslich wird der Durchschnitt der Genauigkeitswerte über alle 5 Folds berechnet und als Fließkommazahl mit zwei Nachkommastellen ausgegeben.\n",
    "\n",
    "Dieser Codeabschnitt zeigt, wie man eine Kreuzvalidierung durchführt, um die Robustheit und Generalisierbarkeit eines maschinellen Lernmodells zu bewerten.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Ausführen einer 5-fache Kreuzvalidierung\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "# Durchschnittliche Genauigkeit über alle 5 Folds ausgeben\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufbau und Training eines Neuronalen Netzwerks mit TensorFlow\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">\n",
    "\n",
    "In diesem Codeabschnitt wird ein neuronales Netzwerkmodell mit der TensorFlow-Bibliothek definiert, kompiliert und trainiert.\n",
    "\n",
    "1. Zuerst wird das Modell als eine Sequenz von Schichten mit der `Sequential` Funktion definiert.\n",
    "2. Anschliessend werden drei Schichten zum Modell hinzugefügt. Die erste Schicht hat 64 Neuronen und verwendet die ReLU (Rectified Linear Unit) Aktivierungsfunktion. Die zweite Schicht hat 32 Neuronen und verwendet ebenfalls die ReLU Aktivierungsfunktion. Die letzte Schicht hat nur ein Neuron und verwendet die Sigmoid Aktivierungsfunktion, die sich gut für binäre Klassifikationsprobleme eignet.\n",
    "3. Dann wird das Modell kompiliert, wobei der Adam-Optimierer, die Binary Cross-Entropy Loss Funktion und Accuracy als Metrik verwendet werden.\n",
    "4. Schliesslich wird das Modell mit den Trainingsdaten trainiert. Es werden 10 Epochen verwendet und die Batch-Größe beträgt 32. Ausserdem werden die Testdaten als Validierungsdatensatz verwendet, um die Leistung des Modells während des Trainings zu überwachen.\n",
    "\n",
    "Dieser Codeabschnitt demonstriert den gesamten Prozess des Aufbaus und Trainierens eines neuronalen Netzwerkmodells mit TensorFlow für ein binäres Klassifikationsproblem.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren des Modells\n",
    "model = Sequential()\n",
    "\n",
    "# Hinzufügen von Schichten\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid')) # Verwenden Sie 'softmax' für mehr als zwei Klassen\n",
    "\n",
    "# Kompilieren des Modells\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Verwenden Sie 'categorical_crossentropy' für mehr als zwei Klassen\n",
    "\n",
    "# Trainieren des Modells\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
